# Installing the required packages for R:
# This is to be done in the R command line and not in R studio
#source("https://www.Bioconductor.org/biocLite.R")
#biocLite("BiocUpgrade")


Getting started in R: Set the working directory > setwd("~/") Check version installed
```{r global_options, include=FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60), tidy = TRUE, fig.align='center')
```


This will help us when finding our files to source functions:
```{r sourcing_my_functions}
install.packages("rprojroot")
library(rprojroot)
# We specify ours is an RStudio project
# The root object contains a function that will help us locate our package r files
# regarless of our current working directory
root <- rprojroot::is_rstudio_project
scriptsPath <- root$make_fix_file(".")("R")
scripts  <- dir(root$find_file("R", path = root$find_file()))
scriptsl <- paste(scriptsPath, scripts, sep = "//")
lapply(scriptsl, source)
```

User: 
Define the path to the shared folder where the main working directory will be.
```{r setting_the_main_directory, cache=TRUE}
sharedPath <- "/home/CFIA-ACIA/girouxeml/PIRL_working_directory/"
```

User:
Specify the name and path of the csv file you would like to use for generation of the 
metadata file if not using the csv file generated by sequencing service:
*Note: I suggest saving the file in the references folder in the shared path. Getting more specific
and making a directory just for these tables is fine too - but the table will be copied to the new
directory for this analysis later in the script. It's just important to have a raw, untouched 
reference for this data kept in a place where you will remember and keep all the other ones you use.
```{r metadata_name_and_path, cache=TRUE}
metadataFileAlternate <- "lachnellula_metadata_IonT_2017.csv"
metadataPath <- paste(sharedPath, "References/", metadataFileAlternate, sep = "")
```

User:
Define the the folder in the shared folder that will hold the analyses of the 
time-course/dataset you will be working with. In our case, we have two different 
time-course experiments, Oosporogenesis and Oospore Conversion. Below we set 
which one the script will run analyses for. We also get the user to specify what 
the name of the directory that will hold the reads will be. In the case below, 
we are calling the sequencing data directory (seqDataDir) "MiSeq_data_Sci2" 
because sequencing of oospore conversion time-course reads was done on the 
in-house MiSeq and was the second run we had done on this instrument for the 
overall project. We also added Sci2 because the sequencing libraries were made 
on the SciClone robotics instrument, and also represent the second time we 
generated libraries on that instrument:
```{r analysis_and_sequence_directory, cache=TRUE}
analysis   <- "Lachnellula_species_GenomeAn_IonTorrent_2017/"
seqDataDir <- "IonTorrent_data_2017"
```

User needs to specify the adapter sequences attached to the sequencing 
reads. This will depend on how the libraries were prepared. Specify which sequencing platform was 
used to generate the reads: 
1 - MiSeq
2 - HiSeq
3 - Ion Torrent

Also specify the library layout - paired-end or single reads (Illumina only):
1 - single
2 - paired-end
```{r sequencing_platform_used, cache=TRUE}
platform      <- 3
libraryLayout <- 1
```

User:
Specify the read-processing and quality assessment tool to be used.
1 - FastQC
2 - PrinSeq
```{r read_processing_tool, cache = TRUE}
readProc <- 2
```

User:
Specify which tool to use for assembly:
1 - Newbler (for 454 and Ion Torrent reads)
2 - SPADES (for Illumina reads, mainly)
```{r assembler, cache = TRUE}
assembler <- 2
```

User:
The following paths are to directories where the references, tools and general 
requirements are located, this depends on the directories actually having been 
put there:
```{r user_tools_and_references_dir, cache=TRUE}
toolsDirPath     <- paste(sharedPath, "tools/", sep = "")
referencesPath   <- paste(sharedPath, "References/", sep = "")
fastqPEValidatorPath <- paste(toolsDirPath, "FastqPairedEndValidator.pl", sep = "")
```

We prepared our libraries using the Mondrian and SciClone with library 
kits, instruments and kits by NuGen. NuGen kits are designed to work with 
Illumina sequencing platforms and generate libraries with the sequence 
structure:

5' AATGATACGGCGACCACCGAGATCTACACTCTTTCCCTACACGACGCTCTTCCGATCT 
   (N) 
   AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC <- region to select as forward adapter
   XXXXXX 
   ATCTCGTATGCCGTCTTCTGCTTG 3'
   
3' TTACTATGCCGCTGGTGGCTCTAGATGTGAGAAAGGGATGTGCTGCGAGAAGGCTAGA 
   (N) 
   TCTAGCCTTCTCGTGTGCAGACTTGAGGTCAGTG <- region to select as reverse adapter
   XXXXXX 
   TAGAGCATACGGCAGAAGACGAAC 5'

Where each string of ‘X’ is the unique 4-, 6, or 8-base barcode from the 
L2 adaptor mix of the library construction system (where applicable) 
and (N) is the library insert.

We will need to remove any adapter sequences from our reads. We will
be doing this with SeqPrep. SeqPrep specifies that the user must first 
ensure the adapter sequences they choose are correct by doing a "grep"
on the reads first:

Before running SeqPrep make sure to check that the program's defaults 
are indeed the adapters you are looking for. Try copying the default 
forward adapter from this file and grep it against your reads doing a 
word count, also try the same with the reverse adapter with grep. You 
should see some hits. You can also try using (and validating with grep) 
-A GATCGGAAGAGCACACG -B AGATCGGAAGAGCGTCGT as parameters. To find a 
list of Illumina adapter sequences you should write to Illumina tech 
support TechSupport@illumina.com (they do not like people to share the 
list of sequences outside of their institution).

Chose about 20bp of an adapter sequence where:
1. You see the most hits with grep
2. When you run a command like:
   cat Lane2_0d_2.fastq | head -n 1000000 | grep "INSERT ADAPTER HERE" | head 
   you see the adapter sequence show up at the beginning of a few reads. 
   Also the -A and -B arguments should be as they show up in your data, 
   SeqPrep searches directly for these sequences without doing reverse 
   complementing.
3. Check the forward and reverse and make sure that you have roughly the 
   same number of hits via a command to count hits like: 
   cat Lane2_0d_2.fastq | head -n 1000000 | grep "INSERT ADAPTER HERE" | wc -l
```{r setting_seq_adapters_based_on_platform, cache=TRUE}
fwdAdapHiSeq    <- "AGATCGGAAGAGCACACGTCTGAACTCCAGTCA"  # HiSeq, Genome Quebec
revAdapHiSeq    <- "AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT"  # HiSeq, Genome Quebec

# Notice that the adapter sequences we chose when processing the MiSeq reads
# is from the same region we are choosing for our HiSeq reads, only shorter:
fwdAdapMiSeq <- "AGATCGGAAGAGCACAC"   # MiSeq
revAdapMiSeq <- "AGATCGGAAGAGCGTCGT"  # MiSeq

fwdAdapIonT  <- "CCATCTCATCCCTGCGTGTCTCCGACTCAG" # "A" adapter sequence for IonTorrent
revAdapIonT  <- "CCTCTCTATGGGCAGTCGGTGAT"        # "trP1" adapter sequence for IonTorrent

if(platform == 1){
    fwdAdap <- fwdAdapMiSeq
    revAdap <- revAdapMiSeq
    print('Your sequencing platform was set to the Illumina MiSeq')} else if 
(platform == 2){
    fwdAdap <- fwdAdapHiSeq
    revAdap <- revAdapHiSeq
    print('Your sequencing platform was set to the Illumina HiSeq')} else if
(platform == 3){
    fwdAdap <- fwdAdapIonT
    revAdap <- revAdapIonT
    print('Your sequencing platform was set to the Ion Torrent')} else {
        rm(fwdAdap, revAdap)
        print('You either didn\'t specify a valid sequence platform, or none was provided')
    }
fwdAdap
revAdap
```

The user does not alter the variables below. 
```{r settingToQsubWithinRStudio, cache=TRUE}
Sys.setenv(PATH=paste('/opt/gridengine/bin/linux-x64',Sys.getenv('PATH'),sep= ':'))
```


The following chunk will integrate the user-defined variables from the previous chunk into the script.
```{r creating_dir_for_analysis, cache=TRUE}
# Create the analysis directory:
dir.create(paste(sharedPath, analysis, sep = ""), 
           showWarnings = TRUE, 
           recursive    = FALSE)

# Set the path to the analysis directory:
sharedPathAn <- paste(sharedPath, analysis, sep="")

# Create fastq directory in sharedPath folder based on "seqDataDir":
dir.create(paste(sharedPathAn, seqDataDir, sep = ""), 
           showWarnings = TRUE, 
           recursive    = FALSE)

# Set the path the fastq directory:
pathFastq <- paste(sharedPathAn, seqDataDir, "/", sep = "")
```

Path to the biocluster-installed tools:
```{r biocluster_tools_paths, cache=TRUE}
tophat2Path      <- "/opt/bio/tophat/bin/tophat2"
bowtie2BuildPath <- "/opt/bio/bowtie2/bowtie2-build"
starPath         <- "/opt/bio/STAR/STAR"
htseqCountPath   <- "/opt/bio/HTSeq/bin/htseq-count"
prinSeqPath      <- "/opt/bio/prinseq-lite/prinseq-lite"
prinSeqGraphPath <- "/opt/bio/prinseq-lite/prinseq-graphs"
samtools1Path    <- "/opt/bio/samtools1/bin/samtools1"
seqPrepPath      <- "/opt/bio/SeqPrep/SeqPrep"
itsxPath         <- "/home/CFIA-ACIA/girouxeml/prog/miniconda/bin/ITSx"
hmmDBITSxPath    <- "/home/CFIA-ACIA/girouxeml/prog/miniconda/bin/ITSx_db/HMMs/"
```

Bowtie:
Creating Bowtie reference index from the fasta file: Not done for Lachnellula willkommii
```{r bowind eval = FALSE, echo = FALSE, include = FALSE, cache=TRUE}
bowind <- "lachwiiRef"

cmd    <- paste(bowtie2BuildPath, 
                " -f ", pyuuRefPath, " ", paste(referencesPath, bowind, sep = ""),
                sep = "")
cmd
#system(cmd)
```

Below the metadata file specified by the user is copied into the analysis folder and read into R:
```{r copy_metadata_to_analysis_dir_and_read, cache=TRUE}

cmd      <- paste("cp ", metadataPath, " ", sharedPathAn, sep = "")
system(cmd)
metadata <-  read.table(paste(sharedPathAn, metadataFileAlternate, sep = ""),
                        sep = ",", header = TRUE, comment.char = "", quote = "", as.is = TRUE)
if(libraryLayout == 2){
    metadata$BaseCallsName <- paste(metadata$LibraryName, "_", 
                                    metadata$ReadDirection, ".fastq", sep = "")
} else if
(libraryLayout != 2){
    metadata$BaseCallsName <- paste(metadata$LibraryName, ".fastq", sep = "")
}
```

This function will delete qsub temp files that are the sub.e* outputs (error outputs)
```{r}
X <- function(path, prefixSub) {
  system(paste("find ", path, prefixSub, "/", prefixSub, "*", ".sub.e", "*", " -delete ", sep = ""))
}
```

Copy and gunzip files:
```{r copyRaw_gunzip, cache=TRUE}
# Copy all files using multiple processors (this is a lot faster than the above)
# cmd <- with(metadata, paste("cp ", 
#                 FastqFilePath," ", 
#                 sharedPathAn, seqDataDir, "/", basename(FastqFilePath),"\n",
#                 " gunzip ", 
#                 sharedPathAn, seqDataDir, "/", basename(metadata$FastqFilePath),
#                 sep=""))
# OR

#If you want to rename the files that you are copying over, do the following.
#It is based on what you put under BaseCallsName in your adapted metadata csv.
cmd <- with(metadata, paste("cp ", 
                FastqFilePath," ", 
                sharedPathAn, seqDataDir, "/", metadata$BaseCallsName, ".gz","\n",
                " gunzip ", 
                sharedPathAn, seqDataDir, "/", metadata$BaseCallsName, ".gz",
                sep=""))

# Generate qsub and bash files to complete the commands above:
prefix <- "A_copy_unzip" 
suffix <- ".sub"
MakeQsubs(cmd, prefix, suffix)
```

Clean-up step:
Remove the output files while keeping the qsub and bash file:
```{r, cache=TRUE}
RemoveQsubTempFiles(sharedPathAn, prefix)
```

Add the name of the raw fastq files to the metadata table
```{r, cache=TRUE}
if(libraryLayout == 2){
    for(k in 1:nrow(metadata)){
        metadata$RawFastq <- paste(metadata$LibraryName, "_", metadata$ReadDirection, ".fastq", sep = "")}
    } else if
(libraryLayout != 2){
    for(k in 1:nrow(metadata)){
        metadata$RawFastq <- paste(metadata$LibraryName, ".fastq", sep = "")
    }
}
```
The following will create a metadata table based on the library layout - paired-end or single reads.
Read pairs will be collapsed to one row per pair.
```{r, cache=TRUE}
library("reshape2")
if(libraryLayout == 2){
    metadataRawPairs <- dcast(data = metadata, LibraryName 
                              ~ ReadDirection, value.var = "BaseCallsName", FUN = c)
    metadataRawPairs$ShortName <- paste(metadataRawPairs$LibraryName)
    metadataRaw <- metadataRawPairs} else if
(libraryLayout != 2){
    metadata$ShortName <- paste(metadata$LibraryName)
    metadataRaw <- metadata} else {
        cat(c("Error: Either an incorrect number was entered for library", "\n",
              "layout (single reads = 1 and paired-end reads = 2), or no selection was", "\n",
              "specified. The default will be to consider reads single.", sep = ""))
        metadata$ShortName <- paste(metadata$LibraryName)
        metadataRaw <- metadata
        }
# We can get more specific if we want, such as when there are many samples and/or a time-course:

# metadataRawPairs$ShortName <- paste(metadataRawPairs$Condition, 
#                                     metadataRawPairs$TimePoint, 
#                                     metadataRawPairs$RNASeq_Replicate, sep = ".")
```

Looking at our raw data:
Ensure that if reads are paired-end, that they are correctly paired and ordered between read 1 and
read 2 fastq files.
```{r, cache=TRUE}
# Run FastqPairedEndValidator for the first time on the raw read pairs:
if(libraryLayout == 2){
    prefix <- "B_Validator"
    cmd <-  with (metadataRaw, paste(fastqPEValidatorPath,
                                     " ", pathFastq, R1, " ", pathFastq, R2, sep = ""))
    suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix)} else if
(libraryLayout != 2){
        cat(c("Fastq pairend end validator will not be used because you did not specify that", "\n",
              "your reads are paired end", sep = ""))
        }
#system("qstat")
```

If reads were paired-end, and the Fastq Paired-End validator was run, the output of each pair will
be displayed on the console:
```{r, echo=FALSE, cache=TRUE}
if(libraryLayout == 2){
    for (k in 1:nrow(metadataRaw)) {
        cat(c(k, metadataRaw$R1[k], metadataRaw$R2[k]))
        system(paste("cat ", sharedPathAn, prefix, "/", prefix, k, suffix, ".o*", sep = ""))
        cat("\n")}} else if
(libraryLayout != 2){
    cat(c("The fastq pairend end validator was not used because you specified that your", "\n",
              "reads are not paired end", sep = ""))
        }
```
To remove the output files after you are done:
```{r, echo=FALSE, cache=TRUE}
RemoveQsubTempFiles(sharedPathAn, prefix)
```

Look at the raw fastq graphs prior to adapter removal, QA, and other processing
using PrinSeq graph reports of raw reads, 
Step 1: .gd file generation:
```{r, echo=FALSE, cache=TRUE}
prefix <- "C_PrinSeq_rawGraphs"
if(libraryLayout == 2){
    cmd <- MakePrinSeqGraphFiles(metadataRaw, metadataRaw$R1, prefix, "rawGraphs", metadataRaw$R2)
    suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix)} else if
(libraryLayout != 2){
    cmd <- MakePrinSeqGraphFiles(metadataRaw, metadataRaw$RawFastq, prefix, "rawGraphs")
    suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix)
}
```

Add the name of the raw graphs .gd files to the 
metadata table:
```{r echo = FALSE, cache=TRUE}
for(k in 1:nrow(metadataRaw)){
  metadataRaw$RawGraphName <- paste(metadataRaw$LibraryName, ".rawGraphs.gd", sep = "") 
}
```

You can follow the progress of the PrinSeq graph generation run with the following:
```{r, echo=FALSE, cache=TRUE}
for(k in 1:nrow(metadataRaw)){
    cat(c(k, metadataRaw$LibraryName[k]))
    cat("\n")
    system(paste("cat ", sharedPathAn, prefix, "/", prefix, k, suffix, ".e*", sep = ""))
    cat("\n")}
```

PrinSeq graph reports of raw reads. 
Step 2: html file generation:
```{r echo = FALSE, cache = TRUE}
prefix2 <- "C2_PrinSeqRawGraphs"
cmd <- MakePrinSeqHTML(metadataRaw, prefix, metadataRaw$RawGraphName)
suffix <- ".sub"  
MakeQsubs(cmd, prefix2, suffix)
```
If ever you need to upload the .gd file on the PrinSeq web server to generate the html files:
http://edwards.sdsu.edu/cgi-bin/prinseq/prinseq.cgi?report=1 
You can follow the progress of the PrinSeq graph generation run with the following:
```{r, echo=FALSE, cache=TRUE}
for(k in 1:nrow(metadataRaw)){
    cat(c(k, metadataRaw$LibraryName[k]))
    cat("\n")
    system(paste("cat ", sharedPathAn, prefix2, "/", prefix2, k, suffix, ".e*", sep = ""))
    cat("\n")}
```

To remove the output files after you are done:
```{r}
RemoveQsubTempFiles(sharedPathAn, prefix2)
```

Merging and adapter removal before processing:
Adapter detection:
```{r echo = FALSE, cache = TRUE}
if(libraryLayout == 2){
for (k in 1:nrow(metadataRaw)) {
  cat(c(k, metadataRaw$R1[k]))
  system(paste("cat ", pathFastq, metadataRaw$R1[k], " | head -n 1000000 | grep '", 
               fwdAdap, "' | wc -l ", sep = ""))
  cat("\n")
  }
    for (k in 1:nrow(metadataRaw)) {
      cat(c(k, metadataRaw$R2[k]))
      system(paste("cat ", pathFastq, metadataRaw$R2[k], " | head -n 1000000 | grep '", 
                   revAdap, "' | wc -l ", sep = ""))
      cat("\n")
      } 
    } else if
(libraryLayout != 2){
    for (k in 1:nrow(metadataRaw)) {
        cat(c(k, metadataRaw$R1[k]))
        system(paste("cat ", pathFastq, metadataRaw$RawFastq[k], " | head -n 1000000 | grep '",
                     fwdAdap, "' | wc -l ", sep = ""))
        cat("\n")
    }
}
```


Adapter removal:
```{r}
prefix <- "D_SeqPrep_adapter_Removal"
if(libraryLayout == 2){
    cmd <- with(metadataRaw, 
            paste(seqPrepPath, 
                  " -f ", pathFastq, R1,
                  " -r ", pathFastq, R2,
                  " -1 ", pathFastq, LibraryName, ".adapRem.R1.fastq.gz",
                  " -2 ", pathFastq, LibraryName, ".adapRem.R2.fastq.gz",
                  " -A ", fwdAdap,
                  " -B ", revAdap,
                  " -s ", pathFastq, paste(LibraryName, ".adapRemMerged.fastq.gz", sep = ""),
                  sep = ""))} else if 
(libraryLayout != 2){
    cmd <- with(metadataRaw, 
            paste(seqPrepPath, 
                  " -f ", pathFastq, RawFastq,
                  " -1 ", pathFastq, LibraryName, ".adapRem.fastq.gz",
                  " -A ", fwdAdap,
                  sep = ""))
}
suffix <- ".sub"; cat(bashDirections); MakeQsubs(cmd, prefix, suffix)
```

To show the output of each pair on the console in Rstudio:
```{r}
if(libraryLayout == 2){
    for(k in 1:nrow(metadataRaw)) {
      cat(c(k, metadataRaw$R1[k], metadataRaw$R2[k]))
      cat("\n")
      system(paste("tail ", sharedPathAn, prefix, "/", prefix, k, suffix, ".e* | head -n 10" , sep=""))
      cat("\n")
    }
    } else if
(libraryLayout != 2){
    for(k in 1:nrow(metadataRaw)) {
      cat(c(k, metadataRaw$RawFastq[k]))
      cat("\n")
      system(paste("tail ", sharedPathAn, prefix, "/", prefix, k, suffix, ".e* | head -n 10" , sep=""))
      cat("\n")
    }
}
```

To remove the output files after you are done:
```{r}
RemoveQsubTempFiles(sharedPathAn, prefix)
```


Pre-Processing of Raw Reads using PrinSeq::

PrinSeq Processing of Raw reads:
PrinSeq Options Setting:
```{r}
nmax           <- 1
trimLeft       <- 15
trimRight      <- 10
trimTailLeft   <- 5   # Only for RNA-Seq - There should be no poly-tails in DNA
trimTailRight  <- 5  # Only for RNA-Seq - There should be no poly-tails in DNA
trimQualWindow <- 7 # To make this less conservative, increase from 3 to 7
trimQualType   <- "mean"
trimQualRight  <- 10 # Consider reducing from 30 to 10 initially
trimQualLeft   <- 10  # Consider reducing from 30 to 10 initially
trimQualRule   <- "lt"
lcMethod       <- "dust"
lcThreshold    <- 7
outGood        <- "processed_merged"    # Define by user
outBad         <- "null"
minLen         <- 100        # Consider reducing from 100 to 60? initially
```

For the pre-processing with PrinSeq we have three steps, with three 
sets of qsubs each:

1. Processing input raw reads with the PrinSeq trim and fitlering options
2. Generating graph files of the processed reads
3. Generating html files using the graph files to visualize the outputs

For each set of qsubs, the .log, .gd, and .html outputs are sent to the 
first folder that also has the first stage qsub and bash files. 
The processed reads are output to the pathfastq folder.

PrinSeq Processing of merged reads:
First stage of quality pre-processing with PrinSeq:

1-1. 
Filter raw.fastq output from SeqPrep by quality: Note, with 
merging with SeqPrep during adapter removal, the reads are already 
filtered and are of high quality as only the high quality reads can 
be merged.
```{r}
prefix <- "C_PrinSeqGraphQualTrim"

# I revised the function, so look to RNASeq script for how to make this work.
cmd = with(metadata, 
           paste(prinSeqPath,
                 " -fastq ",             pathFastq,  RawFastq,
                 " -trim_qual_window ",  trimQualWindow,
                 " -trim_qual_type ",    trimQualType, 
                 " -trim_qual_right ",   trimQualRight,
                 " -trim_qual_rule ",    trimQualRule,
                 " -out_good ",          paste(pathFastq, LibraryName, ".2processedRaw", sep = ""),
                 " -out_bad  ",          outBad,
                 " -verbose ",
                 " -no_qual_header ",
                 " -log ",               sharedPathAn, prefix, "/", LibraryName, ".2processedRaw.log",
                 sep = ""))

suffix <- ".sub"  
MakeQsubs(cmd, prefix, suffix)
```

Add name of quality-filtered reads .fastq files to the metadata 
tabel:
```{r}
for(k in 1:nrow(metadata)){
  metadata$processed1Fastq <- paste(metadata$LibraryName, ".2processedRaw.fastq", sep = "") 
}
```
To remove the output files after you are done:
```{r}
RemoveQsubTempFiles(sharedPathAn, prefix)
```

Gzip the fastq reads that were INPUT to the PrinSeq processing in step 1-1, 
we won't need them and the next step in 1-2 will be using the OUTPUT processed 
reads. 
```{r}
prefix <- "C_gzipRaw"
cmd <- with(metadata, paste("gzip ", pathFastq, metadata$RawFastq, sep=""))

suffix <- ".sub"  
MakeQsubs(cmd, prefix, suffix)
```

To remove the output files after you are done:
```{r}
RemoveQsubTempFiles(sharedPathAn, prefix)
```

1-2.
Generate PrinSeq graph files (.gd) for the fastq generated previously:
```{r}
prefix <- "C_PrinSeqGraphQualTrim"
prefix2 <- "C2_PrinSeqGraph"

cmd <- MakePrinSeqGraphFiles(metadata, metadata$processed1Fastq,
                             prefix, "2processedRaw")

suffix <- ".sub"  
MakeQsubs(cmd, prefix2, suffix)
```

You can follow the progress of the PrinSeq graph generation run with the following:
```{r, echo=FALSE, cache=TRUE}
for(k in 1:nrow(metadata)){
    cat(c(k, metadata$LibraryName[k]))
    cat("\n")
    system(paste("cat ", sharedPathAn, prefix2, "/", prefix2, k, suffix, ".e*", sep = ""))
    cat("\n")}
```

To remove the output files after you are done:
```{r}
RemoveQsubTempFiles(sharedPathAn, prefix2)
```

Add name of quality-filtered reads .gd files to the metadata tabel:
```{r}
for(k in 1:nrow(metadata)){
  metadata$processed1GraphName <- paste(metadata$LibraryName, ".2processedRaw.gd", sep = "") 
}
```

1-3.
PrinSeq graph reports of first-stage html file generation:
**Instead of putting in the filename manually, refer to metadata for it.
```{r}
prefix3 <- "C3_PrinSeqHtml"
cmd <- MakePrinSeqHTML(metadata, prefix, metadata$processed1GraphName)

suffix <- ".sub"  
MakeQsubs(cmd, prefix3, suffix)
```

To remove the output files after you are done:
```{r}
RemoveQsubTempFiles(sharedPathAn, prefix3)
```

Gzip the fastq reads that were output to the PrinSeq processing in step 2-1, 
input reads stayed compressed - no need to compress those. Had to stop this
testing of submitting by qsub.
```{r}
prefix3 <- "C_gzipProcessed"

cmd <- with(metadata, paste("gzip ", pathFastq, 
                           #  metadata$processed2Fastq,
                            metadata$processed1Fastq, sep = ""))
# suffix <- ".sub"  
MakeQsubs(cmd, prefix3, suffix)
```

To remove the output files after you are done:
```{r}
RemoveQsubTempFiles(sharedPathAn, prefix3)
```

Fourth and LAST stage of quality pre-processing with PrinSeq:
Filtering of reads by complexity (DUST) and minimum length
4-1.
```{r}
prefix <- "D_PrinSeqDustMinLen"

cmd <- with(metadata, 
            paste("zcat ", pathFastq, processed1Fastq, ".gz", " | ",
                  prinSeqPath,
                  " -fastq stdin ",
                  " -min_len ",        minLen,
                  " -lc_method ",      lcMethod,
                  " -lc_threshold ",   lcThreshold,
                  " -out_good stdout ",
                  " -out_bad  ",       outBad,
                  " -verbose ",
                  " -no_qual_header ",
                  " -log ",            paste(sharedPathAn, prefix, "/", LibraryName, ".3processed.log", 
                                             sep = ""),
                  " |  gzip > ", paste(pathFastq, LibraryName, ".3processed.fastq.gz", sep = ""), 
                  sep = ""))


suffix <- ".sub"  
MakeQsubs(cmd, prefix, suffix)
```


```{r}
for(k in 1:nrow(metadata)){
  metadata$processed2Fastq <- paste(metadata$LibraryName, ".3processed.fastq", sep = "") 
}
for(k in 1:nrow(metadata)){
  metadata$finalProcessedPath <- paste(pathFastq, "/", metadata$LibraryName, ".3processed.fastq", sep = "") 
}
```

To remove the output files after you are done:
```{r}
RemoveQsubTempFiles(sharedPathAn, prefix)
```

4-2
Generate PrinSeq graph files (.gd) for the fastq generated the previously:
```{r}
prefix2 <- "D2_PrinSeqDustMinLength"

cmd <- MakePrinSeqGraphFiles2(metadata, metadata$processed2Fastq, prefix, "3processed")

suffix <- ".sub"  
MakeQsubs(cmd, prefix2, suffix)
```

Add name of Dust and MinLen filtered reads .gd files to the metadata tabel:
```{r}
for(k in 1:nrow(metadata)){
  metadata$processed2GraphName <- paste(metadata$LibraryName, ".3processed.gd", sep = "") 
}
```

To remove the output files after you are done:
```{r}
RemoveQsubTempFiles(sharedPathAn, prefix2)
```

```{r}
system("/opt/gridengine/bin/linux-x64/qstat") # Remove qsub temp when qstat returns nothing.
# RemoveQsubTempFiles(sharedPathAn, prefix)
```

Get ITS sequences:
Use FungalITSextractor perl script to get the ITS sequences.
1st: Gunzip the fastq file:
```{r}
prefix <- "F_ITS_fastqGunzip"
cmd <- with(metadata, 
            paste("gunzip ", pathFastq, processed2Fastq, ".gz", sep = ""))
cmd[1]
suffix <- ".sub"  
MakeQsubs(cmd, prefix, suffix)
```
To remove the output files after you are done:
```{r}
RemoveQsubTempFiles(sharedPathAn, prefix)
```

2nd: Convert the Fastq to fasta file format:
```{r}
prefix <- "F2_ITS_fastq2fasta"
cmd    <- with(metadata,
               paste(" seqtk fq2fa ", paste(pathFastq, processed2Fastq, sep = ""),
                     " > ", paste(pathFastq, LibraryName, ".3processed.fasta", sep = ""),
                     sep = ""))
cmd[1]
suffix <- ".sub"  
MakeQsubs(cmd, prefix, suffix)

metadata$processed2Fasta <- paste(metadata$LibraryName, ".3processed.fasta", sep = "")
```

3rd: Use ITSx to extract ITS sequences:
Note: The hmmer version on the biocluster, with the /opt/bio/ITSx/bin/HMMs is outdated and causes ITSx to crash.
I downloaded hmmer-3.1b2 locally in /home/CFIA-ACIA/girouxeml/prog/hmmer-3.1b2 and copied over the /bin/HMMs directory
to the ~/prog/hmmer-3.1b2/bin directory and re-ran the ITSx program with the --reset T option and the -p ~/prog/hmmer-3.1b2/bin/HMM
option to point to the local HMM directory, where I also have write privileges. This allows the ITSx program to 
rebuild the problematic hmm files and successfully run. The command line performed was:
$ perl /opt/bio/ITSx/bin/ITSx -i ../IonTorrent_data_2018/Phra1_7964.3processed.fasta -t F -p ~/prog/hmmer-3.1b2/bin/HMMs/ --save_regions all --table T --reset T --cpu 16 -o Phra1_7964   
After creating the bash files inthe chunk below, I navigate to the directory they are in and run each with:  
$ qsub -pe smp 20 -cwd -S /bin/bash "name of bash file"   
Otherwise the multi-threaded, parallel cpu options are not used with the .sh qsub script.  

*** Over here - ran this on 11June2018, and there is consistently an error - I think I need to update the database used.  
Also note that it takes a long time, and perhaps I should have added the option for threaded on top of --cpu.
```{r}
prefix <- "F3_ITS_ITSx"
cmd    <- with(metadata,
               paste("perl ", itsxPath,
                     " -i ", paste(pathFastq, processed2Fasta, sep = ""),
                     " -p ", hmmDBITSxPath, 
                     " -t F --save_regions all --table T --reset T --cpu 20 --multi_thread T ",
                     " -o ", LibraryName,
                     sep = ""))
cmd[1]
suffix <- ".sub"
MakeQsubs(cmd, prefix, suffix, node)
```

Gather fasta data required for all assemblies:
```{r}
library(data.table)
metadata_dt <- as.data.table(metadata)
setkey(metadata_dt, ScientificName, LibraryName, processed2Fastq, finalProcessedPath)

head(metadata_dt)
key(metadata_dt)
metadataAssembly <- metadata_dt[, c("ScientificName", "LibraryName", "processed2Fastq", "finalProcessedPath"), with=FALSE]
sppAbbrv <- c("Fus3", "Phra1", "Phra2" )
metadataAssembly$SppAbbr <- sppAbbrv
```

Did not complete the mapping for the following - as of June 2018:
Mapping against reference Assemblies:
```{r}
fus3RefAssem   <- "refFastaAssemblyEnsembleFungi_Fusarium_langsethiae_JXCE01.fasta"
fus3RefAssPath <- paste(referencesPath, fus3RefAssem, sep = "")
phraRefAssem   <- "refAssemblyEnsemble_Phytophtora_ramorum_AAQX01.fasta"
phraRefAssPath <- paste(referencesPath, phraRefAssem, sep = "")
```

Assembling with Newbler
Must first gunzip the fastq as Newbler can't use compressed files!
```{r}
prefix <- "GunZipForNewbler"

for(i in 1:nrow(metadataAssembly)){
    cmd <- with(metadataAssembly, 
            paste(" gunzip ", paste(metadataAssembly$finalProcessedPath, ".gz", sep = ""),
                  sep = ""))
}

suffix <- ".sub" 
MakeQsubs(cmd, prefix, suffix)
```

To remove the output files after you are done:
```{r}
RemoveQsubTempFiles(sharedPathAn, prefix)
```

```{r}
newblerPath <- "/opt/bio/454/bin/newbler"
runAssPath  <- "/opt/bio/454/bin/runAssembly"

prefix <- "Assembly_Newbler"
node <- "20"
for(i in 1:nrow(metadataAssembly)){
    cmd <- with(metadataAssembly, 
                paste(runAssPath, 
                      " -o ", paste(sharedPath, analysis, prefix, "/", metadataAssembly$LibraryName, sep = ""),
                      " -cpu ", node, " ",    paste(metadataAssembly$finalProcessedPath, sep = ""), 
                      sep = ""))
}
cmd[1] # cmd to re-run Lari only due to previous run crash   
suffix <- ".sub" 

MakeQsubs(cmd, prefix, suffix)
# MakeQsubs(cmd[1], prefix2, suffix, node) # For re-running Lari due to previous run crash.
```

To remove the output files after you are done:
```{r}
RemoveQsubTempFiles(sharedPathAn, prefix)
```

Add the assembled contigs paths from Newbler to the metadata table:
```{r}
for(k in 1:nrow(metadataAssembly)){
  metadataAssembly$NewblerFnaPath <- paste(sharedPathAn, "Assembly_Newbler/", metadataAssembly$LibraryName,
                                           "/454LargeContigs.fna", sep = "")}
```

Must first gzip the fastq after Newbler assembly to save space!
```{r}
prefix <- "GZipAfterNewbler"

for(i in 1:nrow(metadataAssembly)){
    cmd <- with(metadataAssembly, 
            paste(" gzip ", paste(metadataAssembly$finalProcessedPath, sep = ""),
                  sep = ""))
}
cmd
suffix <- ".sub" 
MakeQsubs(cmd, prefix, suffix)
```
*** Over here!!!
To remove the output files after you are done:
```{r}
RemoveQsubTempFiles(sharedPathAn, prefix)
```

We need to fix the Newbler Assembly contig names:
Key issue I had was that there were sequence descriptions after the sequence name
of each contig inthe assembly files from Newbler, and this was causing the tool to be unable to 
create files from and process any of the sequences. If you want to remove the description and 
if your headers are structured like that: 
"> + id + space + description"

Once this part was removed, the tool worked.

i.e.,

>contig00001 1203988 12890445
agccctgcgatcgatcgggctagctagc

using sed:
$ sed -e 's/^\(>[^[:space:]]*\).*/\1/' Lari_assembly_EG2017.fna > Lari_assembly_EG2017_mod.fna

new format:
>contig00001
agccctgcgatcgatcgggctagctagc

Add the name of the fixed assemblies to the metadata table:
```{r}
metadataAssembly$FixedNewAssemName <- paste(sharedPathAn, "Assembly_Newbler/", metadataAssembly$LibraryName, "/",
                                            metadataAssembly$SppAbbr, "_assemblyLargeContigs_EG2018_mod.fna", sep = "")
```

Print the metadata tables:
```{r}
write.table(metadata, file = file.path(referencesPath, "fus_phra_metadata_processed_june2018.csv"),
            sep = ",", row.names = TRUE, col.names = NA, quote = FALSE)

write.table(metadataAssembly, file = file.path(referencesPath, "fus_phra_metadata_Assembly_june2018.csv"),
            sep = ",", row.names = TRUE, col.names = NA, quote = FALSE)
```

